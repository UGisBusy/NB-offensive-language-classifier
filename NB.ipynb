{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UGisBusy/NB-offensive-language-classifier/blob/master/NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPIcIELv-WPe"
      },
      "source": [
        "## 使用Naive Bayes方法製作不當言語分配器\n",
        "\n",
        "採用資料集：https://huggingface.co/datasets/hate_speech_offensive\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDuOE1jX-WPg"
      },
      "source": [
        "### 資料集處理\n",
        "\n",
        "讀取原資料集，並重新定義類別。 <br>\n",
        "- 將原資料集的**hate speech**與**offensive language**合併為**offensive(1)** <br>\n",
        "- 將原資料集的**neither**重新命名為**neutral(0)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkS3aWgK-WPh",
        "outputId": "02432ee6-5abd-49b5-e2b6-7d4c5a26e5ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset hate_speech_offensive (C:/Users/user/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset size: 24783\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 讀取原資料集\n",
        "raw_datasets = load_dataset('hate_speech_offensive', split='train')\n",
        "\n",
        "# 建立新資料集\n",
        "# 更改標籤定義: 將原先 class 0(hate speech), 1(offensive language) 定義為 label 1，class 2(neither) 定義為 label 0\n",
        "# 將兩種類別的資料分別寫入 text_neutral.txt, text_offensive.txt\n",
        "dataset = []\n",
        "f0 = open('text_neutral.txt', 'w', encoding='utf-8')\n",
        "f1 = open('text_offensive.txt', 'w', encoding='utf-8')\n",
        "for ds in raw_datasets:\n",
        "    if(ds['class'] == 2):\n",
        "        dataset.append({'text': ds['tweet'].replace('\\n', ' '), 'label': 0})\n",
        "        f0.write(ds['tweet'].replace('\\n', ' ') + '\\n')\n",
        "    else:\n",
        "        dataset.append({'text': ds['tweet'].replace('\\n', ' '), 'label': 1})\n",
        "        f1.write(ds['tweet'].replace('\\n', ' ') + '\\n')\n",
        "f0.close()\n",
        "f1.close()\n",
        "\n",
        "print(f'dataset size: {len(dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv28p0Mp-WPi"
      },
      "source": [
        "### 分割訓練資料集與測試資料集\n",
        "\n",
        "手動分割訓練資料集(90%)與測試資料集(10%)。<br>\n",
        "由於是隨機分割，最後再評估成果時會執行多次採平均值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFNgIeHt-WPi",
        "outputId": "4a839469-2b66-4653-ee3f-50c25ea9aaef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data size: 22304\n",
            "test data size: 2479\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "from random import shuffle\n",
        "\n",
        "# 分割 訓練資料集(90%) 與 測試資料集(10%)\n",
        "def split_dataset(dataset, split_ratio=0.9):\n",
        "    train_size = int(len(dataset) * split_ratio)\n",
        "    tmp_dataset = deepcopy(dataset)\n",
        "    shuffle(tmp_dataset)\n",
        "    return tmp_dataset[:train_size], tmp_dataset[train_size:]\n",
        "\n",
        "train_dataset, test_dataset = split_dataset(dataset)\n",
        "print(f'train data size: {len(train_dataset)}')\n",
        "print(f'test data size: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF88ci2P-WPj"
      },
      "source": [
        "### 製作詞袋與過濾詞彙\n",
        "\n",
        "製作詞袋時同時計算|V|，供未來計算用。 <br>\n",
        "\n",
        "用了以下幾種方式過濾詞彙：\n",
        "- 英文轉小寫\n",
        "- 刪除純數字\n",
        "- 將@someone取代為@user\n",
        "- 將#tag取代為tag\n",
        "- 將網址取代為http\n",
        "- 將&#dddd取代為其正確的為字\n",
        "- 去除詞首尾的特殊符號"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHfn65en-WPj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 過濾詞彙\n",
        "def filter_word(raw_word):\n",
        "    # 將英文轉為小寫\n",
        "    word = raw_word.lower()\n",
        "\n",
        "    # 將 純數字 刪除\n",
        "    if(re.match(r'\\d+', word)):\n",
        "        return False\n",
        "\n",
        "    # 將 @XXXX 轉為 @user\n",
        "    if(re.match(r'@.*', word)):\n",
        "        return '@user'\n",
        "    \n",
        "    # 將 #XXXX 轉為 XXXX\n",
        "    if(re.match(r'#.*', word)):\n",
        "        return word[1:]\n",
        "    \n",
        "    # 將 http://XXXX https://XXXX 轉為 http\n",
        "    if(re.match(r'http://.*', word) or re.match(r'https://.*', word) ):\n",
        "        return 'http'\n",
        "    \n",
        "    # 將 &#XXXX 轉為 chr(XXXX)\n",
        "    while(re.match(r'.*&#\\d+', word)):\n",
        "        st = re.search(r'&#\\d+', word).start()\n",
        "        en = re.search(r'&#\\d+', word).end()\n",
        "        word = word[:st] + chr(int(word[st+2:en])) + word[en+1:]\n",
        "\n",
        "    # 將首尾的標點符號去除\n",
        "    while(re.match(r'^[^\\w]', word)):\n",
        "        word = word[1:]\n",
        "    while(re.match(r'.*[^\\w]$', word)):\n",
        "        word = word[:-1]\n",
        "    \n",
        "    return word\n",
        "\n",
        "# 製作詞袋\n",
        "def make_bags(dataset):\n",
        "    bags = {0:{}, 1:{}}\n",
        "    V = 0\n",
        "    for data in dataset:\n",
        "        for raw_word in data['text'].split():\n",
        "            if(not (word:=filter_word(raw_word))):\n",
        "                continue\n",
        "            if(word in bags[data['label']]):\n",
        "                bags[data['label']][word] += 1\n",
        "            else:\n",
        "                if(word not in bags[1-data['label']]):\n",
        "                    V += 1\n",
        "                bags[data['label']][word] = 1\n",
        "    \n",
        "    # 將詞袋依照出現次數排序，並寫入 bag_neutral.txt, bag_offensive.txt\n",
        "    lst = sorted([(data[0], data[1]) for data in bags[0].items()], key=lambda x: (-x[1], x[0]))\n",
        "    open('bag_neutral.txt', 'w', encoding='utf-8').writelines([f'{data[0]} {data[1]}\\n' for data in lst])\n",
        "    lst = sorted([(data[0], data[1]) for data in bags[1].items()], key=lambda x: (-x[1], x[0]))\n",
        "    open('bag_offensive.txt', 'w', encoding='utf-8').writelines([f'{data[0]} {data[1]}\\n' for data in lst])\n",
        "    return bags, V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3-E2ush-WPj"
      },
      "source": [
        "### 使用詞袋進行預測\n",
        "詳細的運算方式在報告裡。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n08S_8I-WPk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 使用詞袋進行預測\n",
        "def predict(sentence, bags, V):\n",
        "    p = [1, 1]\n",
        "    for i in range(2):\n",
        "        p_catgory = len(bags[i]) / (len(bags[0]) + len(bags[1]))\n",
        "        count_catgory = sum(bags[i].values())\n",
        "        for raw_word in sentence.split():\n",
        "            if(not (word:=filter_word(raw_word))):\n",
        "                continue\n",
        "            count = bags[i][word] if(word in bags[i]) else 0\n",
        "            p[i] *= (count + 1) / (count_catgory + V)\n",
        "        p[i] *= p_catgory\n",
        "    return 0 if(p[0] > p[1]) else 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM15J_47-WPk"
      },
      "source": [
        "### 進行實驗\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BCNMtfM-WPk",
        "outputId": "65138ef2-7efd-4695-a81f-b0241671b842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1th result:\n",
            "accuracy:  0.9290036304961679\n",
            "precision: 0.9476879962634283\n",
            "recall:    0.9694218824653608\n",
            "\n",
            "2th result:\n",
            "accuracy:  0.9318273497377975\n",
            "precision: 0.9546742209631728\n",
            "recall:    0.9651551312649165\n",
            "\n",
            "3th result:\n",
            "accuracy:  0.9261799112545381\n",
            "precision: 0.9460227272727273\n",
            "recall:    0.9666182873730044\n",
            "\n",
            "4th result:\n",
            "accuracy:  0.9294070189592578\n",
            "precision: 0.9517607332368548\n",
            "recall:    0.96337890625\n",
            "\n",
            "5th result:\n",
            "accuracy:  0.918918918918919\n",
            "precision: 0.9502392344497608\n",
            "recall:    0.9534325492078732\n",
            "\n",
            "6th result:\n",
            "accuracy:  0.9201290843081887\n",
            "precision: 0.9428982725527831\n",
            "recall:    0.9613502935420744\n",
            "\n",
            "7th result:\n",
            "accuracy:  0.9205324727712787\n",
            "precision: 0.9428571428571428\n",
            "recall:    0.9625668449197861\n",
            "\n",
            "8th result:\n",
            "accuracy:  0.9221460266236385\n",
            "precision: 0.9445241511238642\n",
            "recall:    0.9624756335282652\n",
            "\n",
            "9th result:\n",
            "accuracy:  0.9318273497377975\n",
            "precision: 0.9510421715947649\n",
            "recall:    0.9665024630541872\n",
            "\n",
            "10th result:\n",
            "accuracy:  0.9237595804759984\n",
            "precision: 0.948300622307324\n",
            "recall:    0.960717749757517\n",
            "\n",
            "averge accuracy:  0.9253731343283583\n",
            "averge precision: 0.9480007272621822\n",
            "averge recall:    0.9631619741362986\n"
          ]
        }
      ],
      "source": [
        "# 進行10次實驗\n",
        "N = 10\n",
        "record = {'accuracy': [], 'precision': [], 'recall': []}\n",
        "for i in range(N):\n",
        "    # 分割 訓練資料集(90%) 與 測試資料集(10%)\n",
        "    train_dataset, test_dataset = split_dataset(dataset)\n",
        "    \n",
        "    # 製作詞袋\n",
        "    bags, V = make_bags(train_dataset)\n",
        "    \n",
        "    # 計算準確率、紀錄結果\n",
        "    conf_matrix = [[0, 0], [0, 0]]\n",
        "    for data in test_dataset:\n",
        "        conf_matrix[data['label']][predict(data['text'], bags, V)] += 1\n",
        "    \n",
        "    # 紀錄衡量指標\n",
        "    accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / (conf_matrix[0][0] + conf_matrix[0][1] + conf_matrix[1][0] + conf_matrix[1][1])\n",
        "    precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
        "    recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
        "    record['accuracy'].append(accuracy)\n",
        "    record['precision'].append(precision)\n",
        "    record['recall'].append(recall)\n",
        "\n",
        "    # 輸出 accuracy、precision、recall、F1\n",
        "    print(f'{i+1}th result:')\n",
        "    print(f'accuracy:  {accuracy}')\n",
        "    print(f'precision: {precision}')\n",
        "    print(f'recall:    {recall}')\n",
        "    print()\n",
        "\n",
        "# 輸出平均 accuracy、precision、recall\n",
        "print(f'averge accuracy:  {sum(record[\"accuracy\"])/N}')\n",
        "print(f'averge precision: {sum(record[\"precision\"])/N}')\n",
        "print(f'averge recall:    {sum(record[\"recall\"])/N}')\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}