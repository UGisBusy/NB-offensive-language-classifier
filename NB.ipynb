{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UGisBusy/NB-offensive-language-classifier/blob/master/NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JPIcIELv-WPe"
      },
      "source": [
        "## 使用Naive Bayes方法製作不當言語分配器\n",
        "\n",
        "採用資料集：https://huggingface.co/datasets/hate_speech_offensive\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (1.24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (0.13.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\desktop\\playground\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# 下載必要函示庫\n",
        "!pip install datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gDuOE1jX-WPg"
      },
      "source": [
        "### 資料集處理\n",
        "\n",
        "讀取原資料集，並重新定義類別。 <br>\n",
        "- 將原資料集的**hate speech**與**offensive language**合併為**offensive(1)** <br>\n",
        "- 將原資料集的**neither**重新命名為**neutral(0)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BkS3aWgK-WPh",
        "outputId": "02432ee6-5abd-49b5-e2b6-7d4c5a26e5ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\Desktop\\playground\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Found cached dataset hate_speech_offensive (C:/Users/user/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset size: 24783\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 讀取原資料集\n",
        "raw_datasets = load_dataset('hate_speech_offensive', split='train')\n",
        "\n",
        "# 建立新資料集\n",
        "# 更改標籤定義: 將原先 class 0(hate speech), 1(offensive language) 定義為 label 1，class 2(neither) 定義為 label 0\n",
        "# 將兩種類別的資料分別寫入 text_neutral.txt, text_offensive.txt\n",
        "dataset = []\n",
        "f0 = open('text_neutral.txt', 'w', encoding='utf-8')\n",
        "f1 = open('text_offensive.txt', 'w', encoding='utf-8')\n",
        "for ds in raw_datasets:\n",
        "    if(ds['class'] == 2):\n",
        "        dataset.append({'text': ds['tweet'].replace('\\n', ' '), 'label': 0})\n",
        "        f0.write(ds['tweet'].replace('\\n', ' ') + '\\n')\n",
        "    else:\n",
        "        dataset.append({'text': ds['tweet'].replace('\\n', ' '), 'label': 1})\n",
        "        f1.write(ds['tweet'].replace('\\n', ' ') + '\\n')\n",
        "f0.close()\n",
        "f1.close()\n",
        "\n",
        "print(f'dataset size: {len(dataset)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv28p0Mp-WPi"
      },
      "source": [
        "### 分割訓練資料集與測試資料集\n",
        "\n",
        "手動分割訓練資料集(90%)與測試資料集(10%)。<br>\n",
        "由於是隨機分割，最後再評估成果時會執行多次採平均值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HFNgIeHt-WPi",
        "outputId": "4a839469-2b66-4653-ee3f-50c25ea9aaef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data size: 22304\n",
            "test data size: 2479\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "from random import shuffle\n",
        "\n",
        "# 分割 訓練資料集(90%) 與 測試資料集(10%)\n",
        "def split_dataset(dataset, split_ratio=0.9):\n",
        "    train_size = int(len(dataset) * split_ratio)\n",
        "    tmp_dataset = deepcopy(dataset)\n",
        "    shuffle(tmp_dataset)\n",
        "    return tmp_dataset[:train_size], tmp_dataset[train_size:]\n",
        "\n",
        "train_dataset, test_dataset = split_dataset(dataset)\n",
        "print(f'train data size: {len(train_dataset)}')\n",
        "print(f'test data size: {len(test_dataset)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qF88ci2P-WPj"
      },
      "source": [
        "### 過濾詞彙\n",
        "用了以下幾種方式過濾詞彙：\n",
        "- 英文轉小寫\n",
        "- 刪除純數字\n",
        "- 將@someone取代為@user\n",
        "- 將#tag取代為tag\n",
        "- 將網址取代為http\n",
        "- 將&#dddd取代為其正確的為字\n",
        "- 去除詞首尾的特殊符號"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aHfn65en-WPj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 過濾詞彙\n",
        "def filter_word(raw_word):\n",
        "    # 將英文轉為小寫\n",
        "    word = raw_word.lower()\n",
        "\n",
        "    # 將 純數字 刪除\n",
        "    if(re.match(r'\\d+', word)):\n",
        "        return False\n",
        "\n",
        "    # 將 @XXXX 轉為 @user\n",
        "    if(re.match(r'@.*', word)):\n",
        "        return '@user'\n",
        "    \n",
        "    # 將 #XXXX 轉為 XXXX\n",
        "    if(re.match(r'#.*', word)):\n",
        "        return word[1:]\n",
        "    \n",
        "    # 將 http://XXXX https://XXXX 轉為 http\n",
        "    if(re.match(r'http://.*', word) or re.match(r'https://.*', word) ):\n",
        "        return 'http'\n",
        "    \n",
        "    # 將 &#XXXX 轉為 chr(XXXX)\n",
        "    while(re.match(r'.*&#\\d+', word)):\n",
        "        st = re.search(r'&#\\d+', word).start()\n",
        "        en = re.search(r'&#\\d+', word).end()\n",
        "        word = word[:st] + chr(int(word[st+2:en])) + word[en+1:]\n",
        "\n",
        "    # 將首尾的標點符號去除\n",
        "    while(re.match(r'^[^\\w]', word)):\n",
        "        word = word[1:]\n",
        "    while(re.match(r'.*[^\\w]$', word)):\n",
        "        word = word[:-1]\n",
        "    \n",
        "    return word"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 製作詞袋\n",
        "製作詞袋並計算 |V|、neutral/offensive 資料數。 <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|V| = 22931\n",
            "number of neutral data: 3739\n",
            "number of offensive data: 18565\n"
          ]
        }
      ],
      "source": [
        "# 製作詞袋、同時計算|V|, neutral/ofensive資料比數\n",
        "def make_bags(dataset):\n",
        "    bags = {0:{}, 1:{}}\n",
        "    counts = {0:0, 1:0}\n",
        "    V = 0\n",
        "    for data in dataset:\n",
        "        counts[data['label']] += 1\n",
        "        for raw_word in data['text'].split():\n",
        "            if(not (word:=filter_word(raw_word))):\n",
        "                continue\n",
        "            if(word in bags[data['label']]):\n",
        "                bags[data['label']][word] += 1\n",
        "            else:\n",
        "                if(word not in bags[1-data['label']]):\n",
        "                    V += 1\n",
        "                bags[data['label']][word] = 1\n",
        "\n",
        "    return bags, counts, V\n",
        "\n",
        "bags, counts, V = make_bags(train_dataset)\n",
        "print(f'|V| = {V}')\n",
        "print(f'number of neutral data: {counts[0]}')\n",
        "print(f'number of offensive data: {counts[1]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 詞袋優化\n",
        "\n",
        "使用去除common words改善詞袋效能 <br>\n",
        "刪除同時出現在 neutral前100、offensive前200 的詞彙。<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "common words: ['have', 'you', 'back', 'people', 'there', 'a', 'but', 'is', 'and', 'at', 'the', 'how', '@user', 'an', 'http', 'has', 'all', 'make', 'one', 'for', 'they', \"it's\", 'only', 'them', 'out', \"i'm\", 'by', 'time', 'me', 'amp', 'off', 'can', 'no', 'who', 'when', 'he', 'my', 'know', 'about', 'their', 'to', 'will', 'was', 'new', 'so', 'trash', 'what', 'more', 'be', 'day', 'u', 'rt', 'his', 'too', 'get', 'your', 'look', 'just', 'on', 'or', 'i', 'this', 'why', 'want', 'like', 'as', 'lol', 'love', 'of', 'now', 'do', 'see', 'we', 'with', 'would', 'not', 'in', 'from', 'are', 'go', 'man', 'some', 'if', 'that', 'it', \"don't\", 'got', 'good', 'still', 'up']\n"
          ]
        }
      ],
      "source": [
        "# 去除common words提升效能\n",
        "def optimize_bag(bags):\n",
        "    # 找到正反兩詞袋中出現次數高詞彙\n",
        "    common_words = []\n",
        "    sorted_neutral_bags = sorted([(data[0], data[1]) for data in bags[0].items()], key=lambda x: (-x[1], x[0]))\n",
        "    sorted_offensive_bags = sorted([(data[0], data[1]) for data in bags[1].items()], key=lambda x: (-x[1], x[0]))\n",
        "    neutral_words = set([data[0] for data in sorted_neutral_bags[:100]])\n",
        "    offensive_words = set([data[0] for data in sorted_offensive_bags[:200]])\n",
        "    for word in neutral_words:\n",
        "        if(word in offensive_words):\n",
        "            common_words.append(word)\n",
        "            offensive_words.remove(word)\n",
        "\n",
        "    # 將正反詞袋中出現次數高詞彙刪除\n",
        "    for word in common_words:\n",
        "        bags[0].pop(word)\n",
        "        bags[1].pop(word)\n",
        "    \n",
        "    return bags, common_words\n",
        "    \n",
        "# 將詞袋存檔\n",
        "def save_bags(bags):\n",
        "    # 將詞袋依照出現次數排序，並寫入 bag_neutral.txt, bag_offensive.txt\n",
        "    lst = sorted([(data[0], data[1]) for data in bags[0].items()], key=lambda x: (-x[1], x[0]))\n",
        "    open('bag_neutral.txt', 'w', encoding='utf-8').writelines([f'{data[0]} {data[1]}\\n' for data in lst])\n",
        "    lst = sorted([(data[0], data[1]) for data in bags[1].items()], key=lambda x: (-x[1], x[0]))\n",
        "    open('bag_offensive.txt', 'w', encoding='utf-8').writelines([f'{data[0]} {data[1]}\\n' for data in lst])\n",
        "\n",
        "# 輸出common words範例\n",
        "print(f'common words: {optimize_bag(bags)[1]}', ) \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C3-E2ush-WPj"
      },
      "source": [
        "### 使用詞袋進行預測\n",
        "詳細的運算方式在報告裡。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4n08S_8I-WPk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 使用詞袋進行預測\n",
        "def predict(sentence, bags, counts, V):\n",
        "    p = [1, 1]\n",
        "    for i in range(2):\n",
        "        # P(C)\n",
        "        p_catgory = counts[i] / (counts[0] + counts[1])\n",
        "        \n",
        "        # P(W|C)\n",
        "        count_catgory = sum(bags[i].values())\n",
        "        for raw_word in sentence.split():\n",
        "            if(not (word:=filter_word(raw_word))):\n",
        "                continue\n",
        "            count = bags[i][word] if(word in bags[i]) else 0\n",
        "            p[i] *= (count + 1) / (count_catgory + V)\n",
        "        p[i] *= p_catgory\n",
        "    return 0 if(p[0] > p[1]) else 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tM15J_47-WPk"
      },
      "source": [
        "### 進行實驗\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7BCNMtfM-WPk",
        "outputId": "65138ef2-7efd-4695-a81f-b0241671b842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1th result:\n",
            "accuracy:  0.918112141992739\n",
            "precision: 0.9330543933054394\n",
            "recall:    0.9714424007744433\n",
            "\n",
            "2th result:\n",
            "accuracy:  0.9156918112141993\n",
            "precision: 0.9300411522633745\n",
            "recall:    0.9732057416267943\n",
            "\n",
            "3th result:\n",
            "accuracy:  0.9193223073820089\n",
            "precision: 0.9316712834718375\n",
            "recall:    0.9748792270531401\n",
            "\n",
            "4th result:\n",
            "accuracy:  0.9156918112141993\n",
            "precision: 0.9334883720930233\n",
            "recall:    0.9681620839363242\n",
            "\n",
            "5th result:\n",
            "accuracy:  0.918112141992739\n",
            "precision: 0.9325267566309912\n",
            "recall:    0.9718719689621726\n",
            "\n",
            "6th result:\n",
            "accuracy:  0.9156918112141993\n",
            "precision: 0.9369411764705883\n",
            "recall:    0.9636979670861568\n",
            "\n",
            "7th result:\n",
            "accuracy:  0.9164985881403792\n",
            "precision: 0.9280408542246983\n",
            "recall:    0.9746465138956607\n",
            "\n",
            "8th result:\n",
            "accuracy:  0.9225494150867285\n",
            "precision: 0.9344490934449093\n",
            "recall:    0.975254730713246\n",
            "\n",
            "9th result:\n",
            "accuracy:  0.9128680919725696\n",
            "precision: 0.9298737727910238\n",
            "recall:    0.9678832116788321\n",
            "\n",
            "10th result:\n",
            "accuracy:  0.918918918918919\n",
            "precision: 0.9329920893438809\n",
            "recall:    0.972356935014549\n",
            "\n",
            "average accuracy:  0.9173457039128682\n",
            "average precision: 0.9323078944039767\n",
            "average recall:    0.9713400780741319\n"
          ]
        }
      ],
      "source": [
        "# 進行10次實驗\n",
        "N = 10\n",
        "record = {'accuracy': [], 'precision': [], 'recall': []}\n",
        "for i in range(N):\n",
        "    # 分割 訓練資料集(90%) 與 測試資料集(10%)\n",
        "    train_dataset, test_dataset = split_dataset(dataset)\n",
        "    \n",
        "    # 製作詞袋\n",
        "    bags, counts, V = make_bags(train_dataset)\n",
        "    # bags, _ = optimize_bag(bags)\n",
        "    \n",
        "    # 計算準確率、紀錄結果\n",
        "    conf_matrix = [[0, 0], [0, 0]]\n",
        "    for data in test_dataset:\n",
        "        conf_matrix[data['label']][predict(data['text'], bags, counts, V)] += 1\n",
        "    \n",
        "    # 紀錄衡量指標\n",
        "    accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / (conf_matrix[0][0] + conf_matrix[0][1] + conf_matrix[1][0] + conf_matrix[1][1])\n",
        "    precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
        "    recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
        "    record['accuracy'].append(accuracy)\n",
        "    record['precision'].append(precision)\n",
        "    record['recall'].append(recall)\n",
        "\n",
        "    # 輸出 accuracy、precision、recall、F1\n",
        "    print(f'{i+1}th result:')\n",
        "    print(f'accuracy:  {accuracy}')\n",
        "    print(f'precision: {precision}')\n",
        "    print(f'recall:    {recall}')\n",
        "    print()\n",
        "\n",
        "# 輸出平均 accuracy、precision、recall\n",
        "print(f'average accuracy:  {sum(record[\"accuracy\"])/N}')\n",
        "print(f'average precision: {sum(record[\"precision\"])/N}')\n",
        "print(f'average recall:    {sum(record[\"recall\"])/N}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
