{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UGisBusy/NB-offensive-language-classifier/blob/master/NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JPIcIELv-WPe"
      },
      "source": [
        "## 使用Naive Bayes方法製作不當言語分配器\n",
        "\n",
        "採用資料集：https://huggingface.co/datasets/hate_speech_offensive\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gDuOE1jX-WPg"
      },
      "source": [
        "### 資料集處理\n",
        "\n",
        "讀取原資料集，並重新定義類別。 <br>\n",
        "- 將原資料集的**hate speech**與**offensive language**合併為**offensive(1)** <br>\n",
        "- 將原資料集的**neither**重新命名為**neutral(0)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BkS3aWgK-WPh",
        "outputId": "02432ee6-5abd-49b5-e2b6-7d4c5a26e5ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset hate_speech_offensive (C:/Users/user/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset size: 24783\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 讀取原資料集\n",
        "raw_datasets = load_dataset('hate_speech_offensive', split='train')\n",
        "\n",
        "# 建立新資料集\n",
        "# 更改標籤定義: 將原先 class 0(hate speech), 1(offensive language) 定義為 label 1，class 2(neither) 定義為 label 0\n",
        "# 將兩種類別的資料分別寫入 text_neutral.txt, text_offensive.txt\n",
        "dataset = []\n",
        "f0 = open('text_neutral.txt', 'w', encoding='utf-8')\n",
        "f1 = open('text_offensive.txt', 'w', encoding='utf-8')\n",
        "for ds in raw_datasets:\n",
        "    if(ds['class'] == 2):\n",
        "        dataset.append({'text': ds['tweet'].replace('\\n', ' '), 'label': 0})\n",
        "        f0.write(ds['tweet'].replace('\\n', ' ') + '\\n')\n",
        "    else:\n",
        "        dataset.append({'text': ds['tweet'].replace('\\n', ' '), 'label': 1})\n",
        "        f1.write(ds['tweet'].replace('\\n', ' ') + '\\n')\n",
        "f0.close()\n",
        "f1.close()\n",
        "\n",
        "print(f'dataset size: {len(dataset)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv28p0Mp-WPi"
      },
      "source": [
        "### 分割訓練資料集與測試資料集\n",
        "\n",
        "手動分割訓練資料集(90%)與測試資料集(10%)。<br>\n",
        "由於是隨機分割，最後再評估成果時會執行多次採平均值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HFNgIeHt-WPi",
        "outputId": "4a839469-2b66-4653-ee3f-50c25ea9aaef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data size: 22304\n",
            "test data size: 2479\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "from random import shuffle\n",
        "\n",
        "# 分割 訓練資料集(90%) 與 測試資料集(10%)\n",
        "def split_dataset(dataset, split_ratio=0.9):\n",
        "    train_size = int(len(dataset) * split_ratio)\n",
        "    tmp_dataset = deepcopy(dataset)\n",
        "    shuffle(tmp_dataset)\n",
        "    return tmp_dataset[:train_size], tmp_dataset[train_size:]\n",
        "\n",
        "train_dataset, test_dataset = split_dataset(dataset)\n",
        "print(f'train data size: {len(train_dataset)}')\n",
        "print(f'test data size: {len(test_dataset)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qF88ci2P-WPj"
      },
      "source": [
        "### 過濾詞彙\n",
        "用了以下幾種方式過濾詞彙：\n",
        "- 英文轉小寫\n",
        "- 刪除純數字\n",
        "- 將@someone取代為@user\n",
        "- 將#tag取代為tag\n",
        "- 將網址取代為http\n",
        "- 將&#dddd取代為其正確的為字\n",
        "- 去除詞首尾的特殊符號"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aHfn65en-WPj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 過濾詞彙\n",
        "def filter_word(raw_word):\n",
        "    # 將英文轉為小寫\n",
        "    word = raw_word.lower()\n",
        "\n",
        "    # 將 純數字 刪除\n",
        "    if(re.match(r'\\d+', word)):\n",
        "        return False\n",
        "\n",
        "    # 將 @XXXX 轉為 @user\n",
        "    if(re.match(r'@.*', word)):\n",
        "        return '@user'\n",
        "    \n",
        "    # 將 #XXXX 轉為 XXXX\n",
        "    if(re.match(r'#.*', word)):\n",
        "        return word[1:]\n",
        "    \n",
        "    # 將 http://XXXX https://XXXX 轉為 http\n",
        "    if(re.match(r'http://.*', word) or re.match(r'https://.*', word) ):\n",
        "        return 'http'\n",
        "    \n",
        "    # 將 &#XXXX 轉為 chr(XXXX)\n",
        "    while(re.match(r'.*&#\\d+', word)):\n",
        "        st = re.search(r'&#\\d+', word).start()\n",
        "        en = re.search(r'&#\\d+', word).end()\n",
        "        word = word[:st] + chr(int(word[st+2:en])) + word[en+1:]\n",
        "\n",
        "    # 將首尾的標點符號去除\n",
        "    while(re.match(r'^[^\\w]', word)):\n",
        "        word = word[1:]\n",
        "    while(re.match(r'.*[^\\w]$', word)):\n",
        "        word = word[:-1]\n",
        "    \n",
        "    return word"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 製作詞袋\n",
        "製作詞袋並計算 |V|、neutral/offensive 資料數。 <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|V| = 22895\n",
            "number of neutral data: 3744\n",
            "number of offensive data: 18560\n"
          ]
        }
      ],
      "source": [
        "# 製作詞袋、同時計算|V|, neutral/ofensive資料比數\n",
        "def make_bags(dataset):\n",
        "    bags = {0:{}, 1:{}}\n",
        "    counts = {0:0, 1:0}\n",
        "    V = 0\n",
        "    for data in dataset:\n",
        "        counts[data['label']] += 1\n",
        "        for raw_word in data['text'].split():\n",
        "            if(not (word:=filter_word(raw_word))):\n",
        "                continue\n",
        "            if(word in bags[data['label']]):\n",
        "                bags[data['label']][word] += 1\n",
        "            else:\n",
        "                if(word not in bags[1-data['label']]):\n",
        "                    V += 1\n",
        "                bags[data['label']][word] = 1\n",
        "\n",
        "    return bags, counts, V\n",
        "\n",
        "bags, counts, V = make_bags(train_dataset)\n",
        "print(f'|V| = {V}')\n",
        "print(f'number of neutral data: {counts[0]}')\n",
        "print(f'number of offensive data: {counts[1]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 詞袋優化\n",
        "\n",
        "使用去除common words改善詞袋效能 <br>\n",
        "刪除同時出現在 neutral前100、offensive前200 的詞彙。<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "common words: ['why', 'by', 'know', 'a', 'all', '@user', 'get', 'see', 'them', \"don't\", 'he', 'an', 'out', 'lol', 'back', 'my', 'are', 'amp', \"i'm\", \"it's\", 'got', 'like', 'http', 'and', 'too', 'his', 'when', 'time', 'go', 'new', 'one', 'up', 'good', 'can', 'rt', 'at', 'as', 'trash', 'more', 'with', 'we', 'there', 'your', 'day', 'want', 'i', 'will', 'u', 'for', 'was', 'but', 'do', 'that', 'off', 'how', 'just', 'me', 'has', 'no', 'the', 'in', 'their', 'it', 'still', 'or', 'love', 'be', 'man', 'only', 'some', 'now', 'make', 'about', 'been', 'they', 'what', 'would', 'this', 'is', 'people', 'have', 'who', 'look', 'you', 'of', 'if', 'so', 'not', 'to', 'from', 'on']\n"
          ]
        }
      ],
      "source": [
        "# 去除common words提升效能\n",
        "def optimize_bag(bags):\n",
        "    # 找到正反兩詞袋中出現次數高詞彙\n",
        "    common_words = []\n",
        "    sorted_neutral_bags = sorted([(data[0], data[1]) for data in bags[0].items()], key=lambda x: (-x[1], x[0]))\n",
        "    sorted_offensive_bags = sorted([(data[0], data[1]) for data in bags[1].items()], key=lambda x: (-x[1], x[0]))\n",
        "    neutral_words = set([data[0] for data in sorted_neutral_bags[:100]])\n",
        "    offensive_words = set([data[0] for data in sorted_offensive_bags[:200]])\n",
        "    for word in neutral_words:\n",
        "        if(word in offensive_words):\n",
        "            common_words.append(word)\n",
        "            offensive_words.remove(word)\n",
        "\n",
        "    # 將正反詞袋中出現次數高詞彙刪除\n",
        "    for word in common_words:\n",
        "        bags[0].pop(word)\n",
        "        bags[1].pop(word)\n",
        "    \n",
        "    return bags, common_words\n",
        "    \n",
        "# 將詞袋存檔\n",
        "def save_bags(bags):\n",
        "    # 將詞袋依照出現次數排序，並寫入 bag_neutral.txt, bag_offensive.txt\n",
        "    lst = sorted([(data[0], data[1]) for data in bags[0].items()], key=lambda x: (-x[1], x[0]))\n",
        "    open('bag_neutral.txt', 'w', encoding='utf-8').writelines([f'{data[0]} {data[1]}\\n' for data in lst])\n",
        "    lst = sorted([(data[0], data[1]) for data in bags[1].items()], key=lambda x: (-x[1], x[0]))\n",
        "    open('bag_offensive.txt', 'w', encoding='utf-8').writelines([f'{data[0]} {data[1]}\\n' for data in lst])\n",
        "\n",
        "# 輸出common words範例\n",
        "print(f'common words: {optimize_bag(bags)[1]}', ) \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C3-E2ush-WPj"
      },
      "source": [
        "### 使用詞袋進行預測\n",
        "詳細的運算方式在報告裡。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4n08S_8I-WPk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 使用詞袋進行預測\n",
        "def predict(sentence, bags, counts, V):\n",
        "    p = [1, 1]\n",
        "    for i in range(2):\n",
        "        # P(C)\n",
        "        p_catgory = counts[i] / (counts[0] + counts[1])\n",
        "        \n",
        "        # P(W|C)\n",
        "        count_catgory = sum(bags[i].values())\n",
        "        for raw_word in sentence.split():\n",
        "            if(not (word:=filter_word(raw_word))):\n",
        "                continue\n",
        "            count = bags[i][word] if(word in bags[i]) else 0\n",
        "            p[i] *= (count + 1) / (count_catgory + V)\n",
        "        p[i] *= p_catgory\n",
        "    return 0 if(p[0] > p[1]) else 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tM15J_47-WPk"
      },
      "source": [
        "### 進行實驗\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7BCNMtfM-WPk",
        "outputId": "65138ef2-7efd-4695-a81f-b0241671b842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1th result:\n",
            "accuracy:  0.9229528035498185\n",
            "precision: 0.9417792268281323\n",
            "recall:    0.9683908045977011\n",
            "\n",
            "2th result:\n",
            "accuracy:  0.9245663574021783\n",
            "precision: 0.9434050514499532\n",
            "recall:    0.9683149303888622\n",
            "\n",
            "3th result:\n",
            "accuracy:  0.918515530455829\n",
            "precision: 0.9298162976919454\n",
            "recall:    0.9738529847064628\n",
            "\n",
            "4th result:\n",
            "accuracy:  0.9156918112141993\n",
            "precision: 0.9289033457249071\n",
            "recall:    0.972749391727494\n",
            "\n",
            "5th result:\n",
            "accuracy:  0.9112545381202097\n",
            "precision: 0.9250465549348231\n",
            "recall:    0.9711632453567938\n",
            "\n",
            "6th result:\n",
            "accuracy:  0.9152884227511093\n",
            "precision: 0.9292321924144311\n",
            "recall:    0.9724104549854792\n",
            "\n",
            "7th result:\n",
            "accuracy:  0.9128680919725696\n",
            "precision: 0.9276377217553688\n",
            "recall:    0.97021484375\n",
            "\n",
            "8th result:\n",
            "accuracy:  0.918112141992739\n",
            "precision: 0.923438233912635\n",
            "recall:    0.9800598205383848\n",
            "\n",
            "9th result:\n",
            "accuracy:  0.9108511496571198\n",
            "precision: 0.9288389513108615\n",
            "recall:    0.9663906478324403\n",
            "\n",
            "10th result:\n",
            "accuracy:  0.9140782573618395\n",
            "precision: 0.9269662921348315\n",
            "recall:    0.9720176730486009\n",
            "\n",
            "averge accuracy:  0.9164179104477613\n",
            "averge precision: 0.9305063868157889\n",
            "averge recall:    0.9715564796932219\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# 進行10次實驗\n",
        "N = 10\n",
        "record = {'accuracy': [], 'precision': [], 'recall': []}\n",
        "for i in range(N):\n",
        "    # 分割 訓練資料集(90%) 與 測試資料集(10%)\n",
        "    train_dataset, test_dataset = split_dataset(dataset)\n",
        "    \n",
        "    # 製作詞袋\n",
        "    bags, counts, V = make_bags(train_dataset)\n",
        "    # bags, _ = optimize_bag(bags)\n",
        "    \n",
        "    # 計算準確率、紀錄結果\n",
        "    conf_matrix = [[0, 0], [0, 0]]\n",
        "    for data in test_dataset:\n",
        "        conf_matrix[data['label']][predict(data['text'], bags, counts, V)] += 1\n",
        "    \n",
        "    # 紀錄衡量指標\n",
        "    accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / (conf_matrix[0][0] + conf_matrix[0][1] + conf_matrix[1][0] + conf_matrix[1][1])\n",
        "    precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
        "    recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
        "    record['accuracy'].append(accuracy)\n",
        "    record['precision'].append(precision)\n",
        "    record['recall'].append(recall)\n",
        "\n",
        "    # 輸出 accuracy、precision、recall、F1\n",
        "    print(f'{i+1}th result:')\n",
        "    print(f'accuracy:  {accuracy}')\n",
        "    print(f'precision: {precision}')\n",
        "    print(f'recall:    {recall}')\n",
        "    print()\n",
        "\n",
        "# 輸出平均 accuracy、precision、recall\n",
        "print(f'average accuracy:  {sum(record[\"accuracy\"])/N}')\n",
        "print(f'average precision: {sum(record[\"precision\"])/N}')\n",
        "print(f'average recall:    {sum(record[\"recall\"])/N}')\n",
        "\n",
        "print(predict(\"ur mom so fat\", bags, counts, V))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(predict(\"im happy\", bags, counts, V))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
